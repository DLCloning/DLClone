{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "slanted_dataset_0302.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugT1qQ0XEW8U"
      },
      "source": [
        "### Configuration of the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1MRzBLtex2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4feff2c-219c-4e27-ffdc-e0a12030e0a6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip3 install --upgrade pip\n",
        "!pip install -qU t5\n",
        "\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://bucket_block_completion\" \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "!pip install tensorflow-gcs-config #2.7.0 is broken\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n",
            "     |████████████████████████████████| 153 kB 5.2 MB/s            \n",
            "     |████████████████████████████████| 90 kB 8.3 MB/s             \n",
            "     |████████████████████████████████| 4.9 MB 50.2 MB/s            \n",
            "     |████████████████████████████████| 366 kB 58.6 MB/s            \n",
            "     |████████████████████████████████| 3.1 MB 46.7 MB/s            \n",
            "     |████████████████████████████████| 4.0 MB 54.8 MB/s            \n",
            "     |████████████████████████████████| 1.2 MB 54.9 MB/s            \n",
            "     |████████████████████████████████| 286 kB 73.8 MB/s            \n",
            "     |████████████████████████████████| 59 kB 5.3 MB/s             \n",
            "     |████████████████████████████████| 596 kB 74.1 MB/s            \n",
            "     |████████████████████████████████| 895 kB 71.9 MB/s            \n",
            "     |████████████████████████████████| 3.3 MB 41.7 MB/s            \n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gcs-config in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Running on TPU: grpc://10.22.173.122:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poax9QWkEeMV"
      },
      "source": [
        "### Loading of the tsv files\n",
        "We loaded the tsv files, please be sure to upload them on the bucket and to copy the correct path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "source": [
        "\n",
        "nq_tsv_path = {\n",
        "    \"train\":      'gs://bucket_block_completion/dataset_03_02/train.tsv',\n",
        "    \"validation\": 'gs://bucket_block_completion/dataset_03_02/test.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples = dict(train=389536, validation=15742)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Y8Tje7FiNi"
      },
      "source": [
        "### Preprocess of the dataset\n",
        "In this step we preprocess the dataset.  \n",
        "You have to change the path to vocab files (*vocab_model_path* and *vocab_path*)\n",
        "We're going to preprocess all the tsv file so that T5 can use them for HP tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WK7uMr3zYuQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobLvzL18zzR"
      },
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "\n",
        "# # Set the path of sentencepiece model and vocab files\n",
        "# # Must be the same used for the pre-trained phase\n",
        "vocab_model_path = 'gs://bucket_block_completion/code.model'\n",
        "vocab_path = 'gs://bucket_block_completion/code.vocab'\n",
        "\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP4yvsTe4oAH"
      },
      "source": [
        "## If you have more than one task you have to use the prefix, otherwise it is not mandatory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSGp5FiN1yFQ",
        "outputId": "2a501f32-2f92-4ec6-ba74-93340f1b81f4"
      },
      "source": [
        "prefix=''\n",
        "\n",
        "def nq_dataset_task(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_task(\"train\").take(5)):\n",
        "    print(ex)\n",
        "\n",
        "\n",
        "def preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "        inputs = tf.strings.join([prefix + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label}\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "      \n",
        "t5.data.TaskRegistry.remove('finetuning')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"finetuning\",\n",
        "    dataset_fn=nq_dataset_task,\n",
        "    splits=[\"train\",\"validation\"],\n",
        "    text_preprocessor=preprocessing,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few raw train examples...\n",
            "{'input': b'private static long createLongSeed(byte[] seed) <extra_id_0>', 'output': b'{ if (seed == null || seed.length != SEED_SIZE_BYTES) { throw new IllegalArgumentException(\"Java RNG requires a 64-bit (8-byte) seed.\"); } return BinaryUtils.convertBytesToLong(seed, 0); }'}\n",
            "{'input': b'protected ArrayList<SortablePackageInfo> doInBackground(Object... params) { SharedPreferences prefs = listActivity.getSharedPreferences( MainActivity.PREFSFILE, 0); ArrayList<SortablePackageInfo> ret = new ArrayList<SortablePackageInfo>(); PackageManager pm = listActivity.getPackageManager(); List<PackageInfo> list = pm.getInstalledPackages(0); SortablePackageInfo spitmp[] = new SortablePackageInfo[list.size()]; Iterator<PackageInfo> it = list.iterator(); AnnotationsSource aSource = new AnnotationsSource(listActivity); aSource.open(); int idx = 0; while (it.hasNext()) { PackageInfo info = it.next(); try { ApplicationInfo ai = pm.getApplicationInfo(info.packageName, 0); if ((ai.flags & ApplicationInfo.FLAG_SYSTEM) != ApplicationInfo.FLAG_SYSTEM && (ai.flags & ApplicationInfo.FLAG_UPDATED_SYSTEM_APP) != ApplicationInfo.FLAG_UPDATED_SYSTEM_APP) { spitmp[idx] = new SortablePackageInfo(); spitmp[idx].packageName = info.packageName; spitmp[idx].displayName = pm .getApplicationLabel(info.applicationInfo).toString(); spitmp[idx].installer = pm.getInstallerPackageName(info.packageName); spitmp[idx].appInfo = ai; spitmp[idx].versionCode = info.versionCode; spitmp[idx].version = info.versionName; spitmp[idx].firstInstalled = info.firstInstallTime; spitmp[idx].lastUpdated = info.lastUpdateTime; spitmp[idx].uid = info.applicationInfo.uid; spitmp[idx].dataDir = info.applicationInfo.dataDir; spitmp[idx].comment = aSource.getComment(info.packageName); spitmp[idx].tags=aSource.getTags(info.packageName); spitmp[idx].targetsdk = ai.targetSdkVersion; idx++; } } catch (NameNotFoundException exp) { } } SortablePackageInfo spi[] = new SortablePackageInfo[idx]; System.arraycopy(spitmp, 0, spi, 0, idx); Arrays.sort(spi); for (int i = 0; i < spi.length; i++) <extra_id_0> return ret; }', 'output': b'{ spi[i].selected = prefs.getBoolean(MainActivity.SELECTED + \".\" + spi[i].packageName, false); ret.add(spi[i]); }'}\n",
            "{'input': b'private ContentValues createTrackerContentValues(final PersistableBusinessObject toTrack, final ActionTypes actionType, final ObjectTypes objectType) <extra_id_0>', 'output': b'{ ContentValues values = new ContentValues(); values.put(DatabaseV5Constants.COL_TRACKER_TIMESTAMP, System.currentTimeMillis()); values.put(DatabaseV5Constants.COL_TRACKER_ENTITY_ID, toTrack.getId()); values.put(DatabaseV5Constants.COL_TRACKER_ENTITY_TYPE, objectType.toString()); values.put(DatabaseV5Constants.COL_TRACKER_ACTION, actionType.toString()); return values; }'}\n",
            "{'input': b'public static SafeHtml formatErrorHtml(String error) { String text; String cssClass; String title = \"\"; if (error == null) <extra_id_0> else { text = CmsAliasMessages.messageStatusError(); title = SafeHtmlUtils.htmlEscape(error); cssClass = STATUS_ERROR; } String html = \"<div class=\\'\" + cssClass + \"\\' title=\\'\" + title + \"\\'>\" + text + \"</div>\"; return SafeHtmlUtils.fromSafeConstant(html); }', 'output': b'{ text = CmsAliasMessages.messageStatusOk(); cssClass = STATUS_OK; }'}\n",
            "{'input': b'public static SafeHtml formatErrorHtml(String error) { String text; String cssClass; String title = \"\"; if (error == null) { text = CmsAliasMessages.messageStatusOk(); cssClass = STATUS_OK; } else <extra_id_0> String html = \"<div class=\\'\" + cssClass + \"\\' title=\\'\" + title + \"\\'>\" + text + \"</div>\"; return SafeHtmlUtils.fromSafeConstant(html); }', 'output': b'{ text = CmsAliasMessages.messageStatusError(); title = SafeHtmlUtils.htmlEscape(error); cssClass = STATUS_ERROR; }'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fe5c8a5e250>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W-GatnV2dLG",
        "outputId": "bff14d0e-5358-48fc-8bf1-decfacb68876"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"finetuning\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'public final void set(String name, String value) { ParamBase p = params.get(name.toLowerCase()); if (p == null) return; switch (p.type) { case CHECK: <extra_id_0> case SPIN: { SpinParam sp = (SpinParam)p; try { int val = Integer.parseInt(value); if ((val >= sp.minValue) && (val <= sp.maxValue)) sp.value = val; } catch (NumberFormatException ex) { } break; } case COMBO: { ComboParam cp = (ComboParam)p; for (String allowed : cp.allowedValues) if (allowed.toLowerCase().equals(value.toLowerCase())) { cp.value = allowed; break; } break; } case BUTTON: break; case STRING: { StringParam sp = (StringParam)p; sp.value = value; break; } } }', 'inputs': array([   12,    44,    20,    55,     5,    31,   103,     9,    26,\n",
            "          82,     8,     7,     3,   880,   431,   196,    11,   469,\n",
            "           4,    33,     5,    98,     4,  1111,    39,    21,    17,\n",
            "         254,    40,    30,     8,    14,    13,   695,    17,   254,\n",
            "           4,   171,     8,     7,   234,     3,  5220,    56, 32099,\n",
            "         234,   275, 11958,    56,     7, 18864,   880,  3154,    11,\n",
            "          17, 24514,   880,     8,   254,    13,    93,     7,    35,\n",
            "         752,    11,   237,     4,  1306,     5,   122,    10,    21,\n",
            "          17,     5,  1102,   453,  3154,     4,   769,   106,     8,\n",
            "          91,    17,  1102,   685,  3154,     4,   532,   106,     8,\n",
            "           8,  3154,     4,   122,    11,   752,    13,     6,    97,\n",
            "          17,  3157,    38,   480,     8,     7,     6,   591,    13,\n",
            "           6,   234,     3, 15969,   935,    56,     7,     3,  4934,\n",
            "         880,  3989,    11,    17,  4934,   880,     8,   254,    13,\n",
            "          50,    17,    31,  2447,    58,  3989,     4,  6402,   466,\n",
            "           8,    21,    17,  6402,     4,  1111,    37,   117,     5,\n",
            "         122,     4,  1111,   459,     7,  3989,     4,   122,    11,\n",
            "        2447,    13,   591,    13,     6,   591,    13,     6,   234,\n",
            "           3,  4307,    56,   591,    13,   234,     3,  1885,    56,\n",
            "           7,    26,   880,  3154,    11,    17,    31,   880,     8,\n",
            "         254,    13,  3154,     4,   122,    11,    82,    13,   591,\n",
            "          13,     6,     6,     6,     1], dtype=int32), 'targets_pretokenized': b'{ CheckParam cp = (CheckParam)p; if (value.toLowerCase().equals(\"true\")) cp.value = true; else if (value.toLowerCase().equals(\"false\")) cp.value = false; break; }', 'targets': array([   7, 2564,  880, 3989,   11,   17,  846,  880,    8,  254,   13,\n",
            "         21,   17,  122,    4, 1111,   37,  117,   28,  225,  425, 3989,\n",
            "          4,  122,   11,   89,   13,   77,   21,   17,  122,    4, 1111,\n",
            "         37,  117,   28,  348,  425, 3989,    4,  122,   11,   76,   13,\n",
            "        591,   13,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'@Override public CFolderContent listFolder( final CPath cpath ) throws CStorageException { CMFolder cmRoot = loadFoldersStructure(); CMFolder cmFolder = cmRoot.getFolder( cpath ); if ( cmFolder == null ) { CMFolder cmParentFolder = cmRoot.getFolder( cpath.getParent() ); if ( cmParentFolder != null ) { CMBlob cmBlob; try { cmBlob = getBlobByName( cmParentFolder, cpath.getBaseName() ); } catch ( ParseException e ) { throw new CStorageException( \"Can\\'t parse blob at \" + cpath, e ); } if ( cmBlob != null ) { throw new CInvalidFileTypeException( cpath, false ); } } return null; } List<CMBlob> cmBlobs = null; try { cmBlobs = listBlobs( cmFolder ); } catch ( ParseException e ) { throw new CStorageException( e.getMessage(), e ); } Map<CPath, CFile> map = new HashMap<CPath, CFile>(); for ( CMFolder childFolder : cmFolder ) { CFile cFile = childFolder.toCFolder(); map.put( cFile.getPath(), cFile ); } for ( CMBlob cmBLob : cmBlobs ) <extra_id_0> return new CFolderContent( map ); }', 'inputs': array([   19,    27,    12,   810,   644,   399,   247,   644,     5,\n",
            "          44,   810,   128,   202,   330,     3,     8,    42,   810,\n",
            "        7145,     7, 14477,   644,  6631,   795,    11,   551,  6634,\n",
            "        1699,    18, 14477,   644,  6631,   644,    11,  6631,   795,\n",
            "           4,    33,   644,     5,   202,   330,     3,    10,    21,\n",
            "          17,  6631,   644,    40,    30,     3,     8,     7, 14477,\n",
            "         644,   202, 17625,   644,    11,  6631,   795,     4,    33,\n",
            "         644,     5,   202,   330,     4,  1479,    16,     3,    10,\n",
            "          21,    17,   202, 17625,   644,    49,    30,     3,     8,\n",
            "           7, 14477,  2774,  6631,  2774,    13,    93,     7,  6631,\n",
            "        2774,    11,    41,  2774,  2506,     5,   202, 17625,   644,\n",
            "           9,   202,   330,     4,  3670,    66,    16,     3,    10,\n",
            "           6,    97,    17,  4069,    57,     3,     8,     7,    78,\n",
            "          24,   810,  7145,     5,    32,  1825,     2,   132,   545,\n",
            "        6148,   953,    32,    34,   202,   330,     9,    57,     3,\n",
            "          10,     6,    21,    17,  6631,  2774,    49,    30,     3,\n",
            "           8,     7,    78,    24,   810,   878,  6444,    38,     5,\n",
            "         202,   330,     9,    76,     3,    10,     6,     6,    14,\n",
            "          30,    13,     6,    85,    25,  5780,  2774,    29,  6631,\n",
            "        2774,    22,    11,    30,    13,    93,     7,  6631,  2774,\n",
            "          22,    11,   247,  2774,    22,     5,  6631,   644,     3,\n",
            "          10,     6,    97,    17,  4069,    57,     3,     8,     7,\n",
            "          78,    24,   810,  7145,     5,    57,     4,   429,    72,\n",
            "          57,     3,    10,     6,   188,    25,   302,   128,     9,\n",
            "         810,   104,    29,   342,    11,    24,   511,    25,   302,\n",
            "         128,     9,   810,   104,   346,    50,    17, 14477,   644,\n",
            "         750,   644,    58,  6631,   644,     3,     8,     7,   810,\n",
            "         104,   202,   104,    11,   750,   644,     4,   314,   302,\n",
            "         644,    18,   342,     4,   120,     5,   202,   104,     4,\n",
            "        1224,    72,   202,   104,     3,    10,     6,    50,    17,\n",
            "       14477,  2774,  6631,   285, 20860,    58,  6631,  2774,    22,\n",
            "           3,     8, 32099,    14,    24,   810,   644,   399,     5,\n",
            "         342,     3,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'{ CBlob cBlob = cmBLob.toCBlob(); map.put( cBlob.getPath(), cBlob ); }', 'targets': array([    7,   810,  2774,   202,  2774,    11,  6631,   285, 20860,\n",
            "           4,   314,   302,  2774,    18,   342,     4,   120,     5,\n",
            "         202,  2774,     4,  1224,    72,   202,  2774,     3,    10,\n",
            "           6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'@SuppressWarnings(\"unchecked\") @CheckForNull protected PUB getPublicKey() <extra_id_0>', 'inputs': array([   19,   582,    28,   932,   118,    19,   302, 11257,    73,\n",
            "           3, 21148,     3, 18630,    16, 32099,     1], dtype=int32), 'targets_pretokenized': b'{ KeyPair keyPair = getKeyPair(); return keyPair == null ? null : (PUB) keyPair.getPublic(); }', 'targets': array([    7,     3,  5837, 19495,    11,    41,  5837,    18,    14,\n",
            "       19495,    40,    30,     3,     2,    30,    58,    17, 21148,\n",
            "           8, 19495,     4, 10774,    18,     6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'public void doDrag_BackRotateBeginNewDrag()<extra_id_0>', 'inputs': array([  12,   20,  396, 2943,   15, 2587, 7631, 2694,  976, 2943,   16,\n",
            "         25, 4837,   15,  111,   15,  148,   29,    1], dtype=int32), 'targets_pretokenized': b'{ _dialer.beginDrag(centerX-1,centerY); DragResult drag1 = _dialer.doDrag(centerX, centerY+1); DragResult drag2 = _dialer.doDrag(centerX - 1, centerY - 1); assertFalse(drag1.isToAnimate()); assertTrue(drag2.isToAnimate()); assertEquals(90 + 45, drag2.getRotation(), 0); }', 'targets': array([    7,     3,    15, 24167,     4,  3090,  2943,     5,  4393,\n",
            "           2,   802,     9,  4393,     2,    10,     3,  2943,   195,\n",
            "        6033,    94,    11,     3,    15, 24167,     4,  1277,  2943,\n",
            "           5,  4393,     2,     9,  3680,     2, 12140,     3,  2943,\n",
            "         195,  6033,    80,    11,     3,    15, 24167,     4,  1277,\n",
            "        2943,     5,  4393,     2,   139,   806,  3680,     2,   139,\n",
            "         581,  2239,     5, 12055,   672,   112,   129, 14860,    39,\n",
            "         813,     5, 12055,   789,   112,   129, 14860,    39,   223,\n",
            "           5,     2,   148,    34,     3,     2,     9,  6033,   789,\n",
            "       14272,    72,   522,     6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'public void request() { Activity activity = mAppUtils.getCurrentActivity(); if (activity instanceof OnActivityResultDelegate.DelegateHost) <extra_id_0> else { ScreenCaptureRequestActivity.request(mContext, mCallback); } }', 'inputs': array([   12,    20,   190,    16,     7,  3203,  1028,    11,  5863,\n",
            "         217,     4,  1134,   435,    18,    21,    17,  1338,   166,\n",
            "           3,   355,   435,   195,  1638,     4,  1638,   939,     8,\n",
            "       32099,    77,     7,     3,  1260, 16243,   435,     4,   365,\n",
            "           5,  1775,     9,  8323,    10,     6,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ ScreenCaptureRequester requester = new ActivityScreenCaptureRequester( ((OnActivityResultDelegate.DelegateHost) activity).getOnActivityResultDelegateMediator(), activity); requester.setOnActivityResultCallback(mCallback); requester.request(); }', 'targets': array([    7,     3,  1260, 16243,   239, 14826,    11,    24,  3203,\n",
            "        1260, 16243,   239,     5,    17,     5,   355,   435,   195,\n",
            "        1638,     4,  1638,   939,     8,  1028,     8,     4,    33,\n",
            "         355,   435,   195,  1638, 10641,    72,  1028,    10, 14826,\n",
            "           4,  6025,   435,   195,   554,     5, 14191,    10, 14826,\n",
            "           4,   365,    18,     6,     1], dtype=int32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0chadICjOT_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkLJ-FpDF8CD"
      },
      "source": [
        "### Hyper Parameter tuning\n",
        "You can run the HP tuning using the following cells.  \n",
        "Please set the correct path of the variable *MODEL_DIR* (the path to save the pretrained model in), *PATH_GIN_FILE* (the gin file configuration for this HP tuning) and *PRETRAINED_DIR* (the folder that contains the pretrained model).  \n",
        "**Keep attention** to change the *pretrained_model_dir* in finetune step (if you are starting the HP tuning from scratch you have to set the value *PRETRAINED_DIR*, if you are restarting the HP tuning from a previous saved checkpoint you have to set the value *MODEL_DIR*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz1a1TxFNKmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ce2206-ae4b-45f3-9fa9-c70d717efb45"
      },
      "source": [
        "## this is useful if you have more than one task\n",
        "# def _rate_num_input_examples(task):\n",
        "#   if \"train\" in task.splits:\n",
        "#     return float(task.num_input_examples(\"train\"))\n",
        "#   elif \"validation\" in task.splits:\n",
        "#     return float(task.num_input_examples(\"validation\"))\n",
        "#   else:\n",
        "#     raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))\n",
        "\n",
        "\n",
        "# t5.data.MixtureRegistry.remove(\"all_tasks\")\n",
        "# t5.data.MixtureRegistry.add(\n",
        "#     \"all_tasks\",\n",
        "#     [\"java_construct\", \"java_token\", \"java_block\", \"android_construct\", \"android_token\", \"android_block\"],\n",
        "#     default_rate=_rate_num_input_examples\n",
        "#      #default_rate=1.0\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7f4a4465d470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iSh7F89Zqb6"
      },
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular\n",
        "\n",
        "from t5 import models\n",
        "\n",
        "MODEL_SIZE = \"small\" \n",
        "\n",
        "# Set the folder where the checkpoints and all the others information will be writed\n",
        "MODEL_DIR = 'gs://bucket_block_completion/finetuning/0302/model'\n",
        "\n",
        "# Specify the pre-trained dir which must contain the pre-trained models, the operative_config.gin file and the checkpoint file as well\n",
        "PRETRAINED_DIR='gs://bucket_block_completion/pretrained_model'\n",
        "\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 5000),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = models.mtf_model.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    learning_rate_schedule = slanted_triangular,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHp5ScE7nf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725a97e5-7859-4dab-be12-ec54b79f3fab"
      },
      "source": [
        "PATH_GIN_FILE = 'gs://bucket_block_completion/finetuning/0302/operative_config_slanted.gin'\n",
        "import gin\n",
        "\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(PATH_GIN_FILE)\n",
        "    #RUN FINE-TUNING\n",
        "    FINETUNE_STEPS = 152000\n",
        "    model.finetune(\n",
        "        mixture_or_task_name=\"finetuning\",\n",
        "        pretrained_model_dir=PRETRAINED_DIR,\n",
        "        finetune_steps=FINETUNE_STEPS\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://bucket_block_completion/finetuning/0302/operative_config_slanted.gin\n",
            "ERROR:root:Path not found: gs://bucket_block_completion/finetuning/0302/operative_config_slanted.gin\n",
            "INFO:root:system_path_file_exists:gs://bucket_block_completion/pretrained_model/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_block_completion/pretrained_model/operative_config.gin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bucket_block_completion/finetuning/0302/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.22.173.122:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.22.173.122:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.22.173.122:8470', '_evaluation_master': 'grpc://10.22.173.122:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fe5c427be90>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.22.173.122:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.22.173.122:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1810774034169195334)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9042037103947890511)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3542882567442889538)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5928999952122037937)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4638369064194734382)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -1460709647798005720)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -5166066105003168913)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -8723739270041601762)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -2618223231019754010)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3807065555584164563)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -8139324777892907038)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:401: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('heads', 'model'), ('vocab', 'model'), ('ensemble', 'ensemble'), ('batch', 'batch'), ('experts', 'batch'), ('d_ff', 'model')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fe5c4525650>\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CviTb69IJUQ_"
      },
      "source": [
        "### Evaluate the performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJYVynoR5hDe"
      },
      "source": [
        "# Use a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = 512\n",
        "model.eval(\n",
        "    mixture_or_task_name=\"finetuning\",\n",
        "    # mixture_or_task_name=\"all_tasks\",\n",
        "    checkpoint_steps=-1 #evaluate only last checkpoint\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}